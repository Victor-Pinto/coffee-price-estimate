{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Costants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = '../data/Donnet_Auctions.xlsx'\n",
    "tl_ini = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generic libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.insert(1, '../shared')\n",
    "from utils import verbosity\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "try:\n",
    "  global df\n",
    "  df = pd.read_excel(file_path)\n",
    "except FileNotFoundError:\n",
    "  print(f\"Error: File not found at {file_path}\")\n",
    "  verbosity(f\"Error: file not found at {file_path}\", tl=tl_ini, level='error')\n",
    "except Exception as e:\n",
    "  verbosity(f\"Error: {e}\", tl= tl_ini, level='error')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop ('Unnamed: 0', axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tratamiento de datos\n",
    "# ------------------------------------------------------------------------------\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Gráficos\n",
    "# ==============================================================================\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "## TODO: Just in Jupyter Notebooks\n",
    "%matplotlib ipympl\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Correlación entre columnas numéricas\n",
    "# ==============================================================================\n",
    "\n",
    "def tidy_corr_matrix(corr_mat):\n",
    "    '''\n",
    "    Función para convertir una matriz de correlación de pandas en formato tidy\n",
    "    '''\n",
    "    corr_mat = corr_mat.stack().reset_index()\n",
    "    corr_mat.columns = ['variable_1','variable_2','r']\n",
    "    corr_mat = corr_mat.loc[corr_mat['variable_1'] != corr_mat['variable_2'], :]\n",
    "    corr_mat['abs_r'] = np.abs(corr_mat['r'])\n",
    "    corr_mat = corr_mat.sort_values('abs_r', ascending=False)\n",
    "    \n",
    "    return(corr_mat)\n",
    "\n",
    "\n",
    "corr_matrix = df.select_dtypes(include=['float64', 'int']).corr(method='pearson')\n",
    "tidy_corr_matrix(corr_matrix).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Heatmap matriz de correlaciones\n",
    "# ==============================================================================\n",
    "fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(4, 4))\n",
    "\n",
    "sns.heatmap(\n",
    "    corr_matrix,\n",
    "    annot     = True,\n",
    "    cbar      = False,\n",
    "    annot_kws = {\"size\": 8},\n",
    "    vmin      = -1,\n",
    "    vmax      = 1,\n",
    "    center    = 0,\n",
    "    cmap      = sns.diverging_palette(20, 220, n=200),\n",
    "    square    = True,\n",
    "    ax        = ax\n",
    ")\n",
    "\n",
    "ax.set_xticklabels(\n",
    "    ax.get_xticklabels(),\n",
    "    rotation = 45,\n",
    "    horizontalalignment = 'right',\n",
    ")\n",
    "\n",
    "ax.tick_params(labelsize = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gráfico de distribución para cada variable numérica\n",
    "# ==============================================================================\n",
    "# Ajustar número de subplots en función del número de columnas\n",
    "fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(7, 4))\n",
    "axes = axes.flat\n",
    "columnas_numeric = df.select_dtypes(include=['float64', 'int']).columns\n",
    "\n",
    "for i, colum in enumerate(columnas_numeric):\n",
    "    sns.histplot(\n",
    "        data    = df,\n",
    "        x       = colum,\n",
    "        stat    = \"count\",\n",
    "        kde     = True,\n",
    "        color   = (list(plt.rcParams['axes.prop_cycle'])*2)[i][\"color\"],\n",
    "        line_kws= {'linewidth': 2},\n",
    "        alpha   = 0.3,\n",
    "        ax      = axes[i]\n",
    "    )\n",
    "    axes[i].set_title(colum, fontsize = 8, fontweight = \"bold\")\n",
    "    axes[i].tick_params(labelsize = 8)\n",
    "    axes[i].set_xlabel(\"\")\n",
    "    axes[i].set_ylabel(\"\")\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.subplots_adjust(top = 0.9)\n",
    "fig.suptitle('Distribución variables numéricas', fontsize = 10, fontweight = \"bold\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generic Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass, field\n",
    "from typing import Optional, Protocol\n",
    "from pydantic import BaseModel\n",
    "\n",
    "\n",
    "class Model(BaseModel):\n",
    "    ...\n",
    "\n",
    "\n",
    "class CreateModel(Protocol):\n",
    "    def create_model(self) -> Model: ...\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear regression\n",
    "\n",
    "based in: Regresión lineal con Python by Joaquín Amat Rodrigo, disponible bajo una licencia Attribution-NonCommercial-ShareAlike 4.0 International (CC BY-NC-SA 4.0 DEED) en https://cienciadedatos.net/documentos/py10-regresion-lineal-python.html\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tratamiento de datos\n",
    "# ==============================================================================\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Gráficos\n",
    "# ==============================================================================\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "## TODO: Solo para Jupyter Notebooks\n",
    "%matplotlib ipympl\n",
    "\n",
    "# Preprocesado y modelado\n",
    "# ==============================================================================\n",
    "from scipy.stats import pearsonr\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import root_mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.formula.api import ols\n",
    "import statsmodels.formula.api as smf\n",
    "from statsmodels.stats.anova import anova_lm\n",
    "from scipy import stats\n",
    "\n",
    "# Configuración matplotlib\n",
    "# ==============================================================================\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "# Configuración warnings\n",
    "# ==============================================================================\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data treatment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# División de los datos en train y test\n",
    "# ==============================================================================\n",
    "\n",
    "usecols=['year', 'score', 'size_30kg_boxes', 'variety', 'rank', 'country']\n",
    "\n",
    "variety_mapping = {\n",
    "    'Caturra':0, \n",
    "    'Bourbon':1, \n",
    "    'Catuai':2, \n",
    "    'Other':3, \n",
    "    'Typica':4, \n",
    "    'Pacamara':5\n",
    "}\n",
    "rank_mapping = {\n",
    "    'first': 0,\n",
    "    'second': 1,\n",
    "    'third': 2,\n",
    "    'other': 3,\n",
    "}\n",
    "country_mapping = {\n",
    "    'bolivia':0,\n",
    "    'brazil':1,\n",
    "    'colombia':2,\n",
    "    'el-salvador':3,\n",
    "    'honduras':4,\n",
    "    'nicaragua':5\n",
    "}\n",
    "\n",
    "df['high_bid'] = df['High_Bid_v2']\n",
    "\n",
    "df['year'] = df['Year']\n",
    "df['score'] = df['Score_v2']\n",
    "df['size_30kg_boxes'] = df['Size_30Kg_boxes_v2']\n",
    "df['variety'] = df.Variety_v2.map(variety_mapping)\n",
    "df['rank'] = df.Rank_v2.map(rank_mapping)\n",
    "df['country'] = df.Country_v2.map(country_mapping)\n",
    "\n",
    "X = df[usecols]\n",
    "y = df['high_bid']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "                                        X,\n",
    "                                        y,\n",
    "                                        train_size   = 0.8,\n",
    "                                        random_state = 1234,\n",
    "                                        shuffle      = True\n",
    "                                    ) \n",
    "\n",
    "# X_train.head()\n",
    "df_train = pd.concat([X_train, y_train], axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creación del modelo utilizando matrices como en scikitlearn\n",
    "# ==============================================================================\n",
    "# A la matriz de predictores se le añade una columna de 1s para el intercept del modelo\n",
    "X_train = sm.add_constant(X_train, prepend=True)\n",
    "\n",
    "model = ols('high_bid ~ year + score + size_30kg_boxes + C(variety) + C(rank) + C(country)', data = df_train)\n",
    "model = model.fit()\n",
    "print(model.summary())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ci_intervals = model.conf_int(alpha=0.05)\n",
    "ci_intervals.columns = ['2.5%', '97.5%']\n",
    "ci_intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diagnóstico errores (residuos) de las predicciones de entrenamiento\n",
    "# ==============================================================================\n",
    "train_prediction = model.predict(exog=X_train)\n",
    "waste   = train_prediction - y_train\n",
    "\n",
    "prediction_test = model.predict(exog=X_test)\n",
    "mae = mean_absolute_error(\n",
    "        y_true  = y_test,\n",
    "        y_pred  = prediction_test,\n",
    "    )\n",
    "\n",
    "verbosity(f\"The value of variable mae is: {mae}\", tl=tl_ini, level='notif')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gráficos\n",
    "# ==============================================================================\n",
    "fig, axes = plt.subplots(nrows=3, ncols=2, figsize=(9, 8))\n",
    "\n",
    "axes[0, 0].scatter(y_train, train_prediction, edgecolors=(0, 0, 0), alpha = 0.4)\n",
    "axes[0, 0].plot([y_train.min(), y_train.max()], [y_train.min(), y_train.max()], 'k--', lw=2)\n",
    "axes[0, 0].set_title('Valor predicho vs valor real', fontsize=10)\n",
    "axes[0, 0].set_xlabel('Real')\n",
    "axes[0, 0].set_ylabel('Predicción')\n",
    "axes[0, 0].tick_params(labelsize = 7)\n",
    "\n",
    "axes[0, 1].scatter(list(range(len(y_train))), waste, edgecolors=(0, 0, 0), alpha=0.4)\n",
    "axes[0, 1].axhline(y=0, linestyle='--', color='black', lw=2)\n",
    "axes[0, 1].set_title('Residuos del modelo', fontsize = 10)\n",
    "axes[0, 1].set_xlabel('id')\n",
    "axes[0, 1].set_ylabel('Residuo')\n",
    "axes[0, 1].tick_params(labelsize = 7)\n",
    "\n",
    "sns.histplot(\n",
    "    data     = waste,\n",
    "    stat     = \"density\",\n",
    "    kde      = True,\n",
    "    line_kws = {'linewidth': 1},\n",
    "    color    = \"firebrick\",\n",
    "    alpha    = 0.3,\n",
    "    ax       = axes[1, 0]\n",
    ")\n",
    "\n",
    "axes[1, 0].set_title('Distribución residuos del modelo', fontsize=10)\n",
    "axes[1, 0].set_xlabel(\"Residuo\")\n",
    "axes[1, 0].tick_params(labelsize = 7)\n",
    "\n",
    "sm.qqplot(\n",
    "    waste,\n",
    "    fit   = True,\n",
    "    line  = 'q',\n",
    "    ax    = axes[1, 1], \n",
    "    color = 'firebrick',\n",
    "    alpha = 0.4,\n",
    "    lw    = 2\n",
    ")\n",
    "axes[1, 1].set_title('Q-Q residuos del modelo', fontsize=10)\n",
    "axes[1, 1].tick_params(labelsize=7)\n",
    "\n",
    "axes[2, 0].scatter(train_prediction, waste, edgecolors=(0, 0, 0), alpha=0.4)\n",
    "axes[2, 0].axhline(y=0, linestyle='--', color='black', lw=2)\n",
    "axes[2, 0].set_title('Residuos del modelo vs predicción', fontsize=10)\n",
    "axes[2, 0].set_xlabel('Predicción')\n",
    "axes[2, 0].set_ylabel('Residuo')\n",
    "axes[2, 0].tick_params(labelsize=7)\n",
    "\n",
    "# Se eliminan los axes vacíos\n",
    "fig.delaxes(axes[2,1])\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.subplots_adjust(top=0.9)\n",
    "fig.suptitle('Diagnóstico residuos', fontsize=12);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tratamiento de datos\n",
    "# ------------------------------------------------------------------------------\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Gráficos\n",
    "# ------------------------------------------------------------------------------\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Preprocesado y modelado\n",
    "# ------------------------------------------------------------------------------\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.tree import plot_tree\n",
    "from sklearn.tree import export_graphviz\n",
    "from sklearn.tree import export_text\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "# Configuración warnings\n",
    "# ------------------------------------------------------------------------------\n",
    "import warnings\n",
    "warnings.filterwarnings('once')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data treatment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 100\n",
    "usecols=['year', 'score', 'size_30kg_boxes', 'variety', 'rank', 'country']\n",
    "\n",
    "df['high_bid'] = df['High_Bid_v2']\n",
    "\n",
    "df['year'] = df['Year']\n",
    "df['score'] = df['Score_v2']\n",
    "df['size_30kg_boxes'] = df['Size_30Kg_boxes_v2']\n",
    "df['variety'] = df['Variety_v2']\n",
    "df['rank'] = df['Rank_v2']\n",
    "df['country'] = df['Country_v2']\n",
    "\n",
    "df_train_dt = df[usecols]\n",
    "\n",
    "# División de los datos en train y test\n",
    "# ------------------------------------------------------------------------------\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "                                        df_train_dt,\n",
    "                                        df['high_bid'],\n",
    "                                        random_state = SEED\n",
    "                                    )\n",
    "\n",
    "def get_dummies_and_concat(df:pd.DataFrame, cols_name_list:list)->pd.DataFrame:\n",
    "    df = pd.concat([df, pd.get_dummies(df[cols_name_list])], axis=1)\n",
    "    df.drop(columns=cols_name_list,inplace=True)\n",
    "    return df\n",
    "\n",
    "categorical_variables = ['variety', 'rank', 'country']     \n",
    "\n",
    "X_train = get_dummies_and_concat(X_train, categorical_variables)\n",
    "X_test = get_dummies_and_concat(X_test, categorical_variables)\n",
    "df_train_dt = get_dummies_and_concat(df_train_dt, categorical_variables)\n",
    "\n",
    "\n",
    "# X_train = pd.concat([X_train, pd.get_dummies(X_train[categorical_variables])], axis=1)\n",
    "# X_train.drop(columns=categorical_variables,inplace=True)\n",
    "\n",
    "X_test.head()\n",
    "# pd.get_dummies(df_train_dt[['variety']])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelo = DecisionTreeRegressor(\n",
    "            max_depth         = 3,\n",
    "            random_state      = SEED\n",
    "          )\n",
    "\n",
    "# Entrenamiento del modelo\n",
    "# ------------------------------------------------------------------------------\n",
    "modelo.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estructura del árbol creado\n",
    "# ------------------------------------------------------------------------------\n",
    "fig, ax = plt.subplots(figsize=(12, 5))\n",
    "\n",
    "print(f\"Profundidad del árbol: {modelo.get_depth()}\")\n",
    "print(f\"Número de nodos terminales: {modelo.get_n_leaves()}\")\n",
    "\n",
    "plot = plot_tree(\n",
    "            decision_tree = modelo,\n",
    "            feature_names = X_test.columns,\n",
    "            class_names   = ['high_bid'],\n",
    "            filled        = True,\n",
    "            impurity      = False,\n",
    "            fontsize      = 10,\n",
    "            precision     = 2,\n",
    "            ax            = ax\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texto_modelo = export_text(\n",
    "                    decision_tree = modelo,\n",
    "                    feature_names = list(X_test.columns)\n",
    "               )\n",
    "print(texto_modelo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Measuring the importance of predictors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importancia_predictores = pd.DataFrame(\n",
    "                            {'predictor': X_test.columns,\n",
    "                             'importancia': modelo.feature_importances_}\n",
    "                            )\n",
    "print(\"Importancia de los predictores en el modelo\")\n",
    "print(\"-------------------------------------------\")\n",
    "importancia_predictores.sort_values('importancia', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Error de test del modelo inicial\n",
    "#-------------------------------------------------------------------------------\n",
    "predicciones = modelo.predict(X = X_test)\n",
    "\n",
    "rmse = mean_squared_error(\n",
    "        y_true  = y_test,\n",
    "        y_pred  = predicciones,\n",
    "       )\n",
    "print(f\"El error (rmse) de test es: {rmse}\")\n",
    "\n",
    "mae = mean_absolute_error(\n",
    "        y_true  = y_test,\n",
    "        y_pred  = predicciones,\n",
    "       )\n",
    "print(f\"El error (mae) de test es: {mae}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
